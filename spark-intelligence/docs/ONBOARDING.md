# ðŸ§  Spark Intelligence â€” Onboarding Guide

## For New Users

### What is Spark Intelligence?

Spark Intelligence is a **self-evolution layer** for AI agents. It continuously captures patterns from your coding interactions, distills them through the EIDOS framework, and feeds intelligence back to your agent â€” making it genuinely smarter over time. Unlike static system prompts, Spark creates a living memory that evolves with you.

It works with **OpenClaw** (or any Claude-based agent) and requires **zero API keys** â€” just Claude OAuth. Set `SPARK_EMBEDDINGS=0` to keep it completely free beyond your normal Claude usage.

### Quick Start

```bash
# One command to install everything:
git clone https://github.com/vibeforge1111/spark-openclaw-installer.git
cd spark-openclaw-installer

# Windows:
.\install.ps1

# Mac/Linux:
chmod +x install.sh && ./install.sh
```

That's it. The installer handles Python deps, OpenClaw, Claude Code CLI, config files, and starts the services. After install, just **code normally** â€” Spark learns in the background.

### What to Expect in Your First 24 Hours

| Timeframe | What Happens |
|-----------|-------------|
| **Hour 0-1** | Spark starts capturing your interactions silently |
| **Hour 1-3** | First patterns detected (communication style, preferences) |
| **Hour 3-6** | First advisory generated â€” your agent starts adapting |
| **Hour 6-12** | Pattern confidence grows, behavior adjustments become visible |
| **Hour 12-24** | EIDOS distillations form â€” deep understanding of your workflow |

You don't need to do anything special. Just work. Spark watches and learns.

### Understanding the Dashboard

Open **http://localhost:8765** (Spark Pulse) to see:

- **Neural Activity** â€” Real-time visualization of learning events
- **Learnings Feed** â€” What Spark has captured from your interactions
- **Pattern Map** â€” Detected behavioral and preference patterns
- **Advisory Log** â€” History of advisories sent to your agent
- **EIDOS View** â€” Deep distillations of your working style

---

## For Power Users

### How the Self-Evolution Loop Works

```
You code with your agent
        â†“
Spark captures interactions (tailer)
        â†“
Pattern detection runs locally (no embeddings needed)
        â†“
Bridge cycle: Claude reviews patterns, creates advisories
        â†“
Advisories written to SPARK_ADVISORY.md
        â†“
Your agent reads advisories and adapts
        â†“
You notice the improvement (or give feedback)
        â†“
Feedback feeds back into the loop
        â†“
Repeat â€” agent gets smarter each cycle
```

### Giving Feedback

Feedback is how you steer Spark's learning. Use `agent_feedback.py`:

```python
from spark.agent_feedback import record_feedback, rate_advisory

# Tell Spark about a preference
record_feedback("User prefers functional style over OOP")

# Rate an advisory (did it help?)
rate_advisory(advisory_id="adv_2026_0210_001", helpful=True, notes="Nailed it")

# Report a correction
record_feedback("User corrected: use 'const' not 'let' by default", signal="correction")
```

Or use the CLI:
```bash
spark learn "User prefers dark mode in all tools"
spark feedback --advisory adv_001 --helpful true
```

### Writing Custom Chips

Chips are pluggable intelligence modules. Create one in `chips/`:

```python
# chips/my_chip.py
from spark.chips.base import Chip

class MyChip(Chip):
    name = "my_custom_chip"
    
    async def process(self, interaction):
        # Your custom pattern detection logic
        if "deadline" in interaction.text.lower():
            return self.signal("deadline_detected", confidence=0.8)
        return None
```

Register in `config/chips.yaml` and restart sparkd.

### Tuning the System

See **[TUNEABLES.md](../TUNEABLES.md)** for all configurable parameters:

- **Memory Gate threshold** (default 0.5) â€” Controls what gets persisted
- **Pattern confidence** â€” Minimum confidence to surface a pattern
- **Bridge cycle interval** â€” How often Claude reviews patterns (default 60 min)
- **Advisory priority weights** â€” What gets flagged as HIGH vs MED vs LOW

### The `advice_action_rate` Metric

This is your key health metric. It measures: **of all advisories generated, what % led to observable behavior change?**

- **>60%**: Excellent â€” Spark is well-tuned to your needs
- **30-60%**: Good â€” Some advisories are noise, consider raising thresholds
- **<30%**: Needs tuning â€” Lower the memory gate or adjust chip weights

Track it on the dashboard or: `spark status --metrics`

---

## For OpenClaw Users

### How Spark Integrates with OpenClaw

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           OpenClaw Agent            â”‚
â”‚                                     â”‚
â”‚  Reads: SPARK_CONTEXT.md            â”‚
â”‚         SPARK_ADVISORY.md           â”‚
â”‚         SPARK_NOTIFICATIONS.md      â”‚
â”‚                                     â”‚
â”‚  Writes: memory/*.md, interactions  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Spark Tailer      â”‚  â† Watches OpenClaw workspace
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   sparkd (:8787)    â”‚  â† Core intelligence engine
    â”‚   Pattern Detection â”‚
    â”‚   EIDOS Framework   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Bridge Worker     â”‚  â† Claude reviews patterns
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Spark Pulse (:8765)â”‚  â† Dashboard
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workspace Files

| File | Purpose | Who Writes | Who Reads |
|------|---------|-----------|-----------|
| `SPARK_CONTEXT.md` | Current learnings, patterns, EIDOS | Spark | Agent |
| `SPARK_ADVISORY.md` | Active advisories (HIGH/MED/LOW) | Spark | Agent |
| `SPARK_NOTIFICATIONS.md` | System notifications | Spark | Agent |
| `HEARTBEAT.md` | What agent checks each heartbeat | You/Installer | Agent |
| `SOUL.md` | Agent identity and personality | You/Onboard | Agent |
| `IDENTITY.md` | Human + agent metadata | Onboard script | Agent |

### Cron Setup for Advisory Review

Option 1 â€” **OpenClaw Cron** (recommended):
```bash
openclaw cron add --every 60m --command "python sparkd.py bridge --once --output openclaw"
```

Option 2 â€” **System crontab**:
```bash
# Run bridge every hour
0 * * * * cd ~/.spark/vibeship-spark-intelligence && SPARK_EMBEDDINGS=0 python3 bridge_worker.py --once
```

Option 3 â€” **HEARTBEAT.md** (agent-driven):
The default HEARTBEAT.md already includes advisory checking. The agent will read advisories every heartbeat (~30 min).

### HEARTBEAT.md Configuration

The installer sets up HEARTBEAT.md to:
1. Read SPARK_ADVISORY.md for new advisories
2. Read SPARK_NOTIFICATIONS.md for system events
3. Check SPARK_CONTEXT.md for updated learnings
4. Periodically feed observations back to Spark

Edit `~/.openclaw/workspace/HEARTBEAT.md` to customize.

### The `spark` CLI Commands

```bash
spark start              # Start sparkd + bridge + pulse
spark stop               # Stop all services
spark status             # Health check + metrics
spark learn "insight"    # Manually teach Spark something
spark advisory "warning" # Create a manual advisory
spark pattern "pattern"  # Report a detected pattern
spark bridge             # Trigger a bridge cycle manually
spark bridge --once      # Run one cycle and exit
spark logs               # Tail sparkd logs
spark config             # Show current config
```

---

## Why SPARK_EMBEDDINGS=0?

This is **critical** and set by default. Here's why:

1. **Cost**: Embeddings require API calls for every piece of text. Over days of coding, this adds up fast.
2. **Not needed**: Spark's pattern detection uses keyword matching + frequency analysis, which works great without embeddings.
3. **Privacy**: No text leaves your machine for embedding â€” everything stays local.
4. **Speed**: No network latency for embedding calls.

The only Claude API usage is the bridge cycle (~1 call/hour), which uses your OAuth session.

If you *want* embeddings for advanced semantic matching, set `SPARK_EMBEDDINGS=1` â€” but understand the cost implications.
